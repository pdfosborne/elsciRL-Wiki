@book{bostromSuperintelligencePathsDangers2016,
  title = {Superintelligence: {{Paths}}, {{Dangers}}, {{Strategies}}},
  shorttitle = {Superintelligence},
  author = {Bostrom, Nick and Bostrom, Nick},
  year = {2016},
  month = apr,
  publisher = {Oxford University Press},
  address = {Oxford, New York},
  urldate = {2024-09-09},
  abstract = {The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence. This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.                                                        ,                The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence. This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.},
  isbn = {978-0-19-873983-8},
  keywords = {general},
  file = {/home/philiposborne/Zotero/storage/Z7VEQJUK/superintelligence-9780198739838.html}
}

@article{dulac-arnoldChallengesRealworldReinforcement2021,
  title = {Challenges of Real-World Reinforcement Learning - Definitions, Benchmarks and Analysis},
  shorttitle = {Challenges of Real-World Reinforcement Learning},
  author = {{Dulac-Arnold}, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
  year = {2021},
  month = sep,
  journal = {Machine Learning},
  volume = {110},
  number = {9},
  pages = {2419--2468},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-05961-4},
  urldate = {2023-12-28},
  abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
  langid = {english},
  keywords = {AppliedReinforcementLearning,EvaluationProtocol,RealWorld,RL,Survey},
  annotation = {271 citations (Semantic Scholar/DOI) [2024-04-23]},
  file = {/home/philiposborne/Zotero/storage/2XEECLMQ/Dulac-Arnold et al. - 2021 - Challenges of real-world reinforcement learning d.pdf}
}

@misc{fengNaturalLanguageReinforcement2024,
  title = {Natural {{Language Reinforcement Learning}}},
  author = {Feng, Xidong and Wan, Ziyu and Yang, Mengyue and Wang, Ziyan and Koushik, Girish A. and Du, Yali and Wen, Ying and Wang, Jun},
  year = {2024},
  month = feb,
  number = {arXiv:2402.07157},
  eprint = {2402.07157},
  publisher = {arXiv},
  urldate = {2024-04-23},
  abstract = {Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Artificial Intelligence,Computation and Language,Language,Machine Learning,RL},
  annotation = {0 citations (Semantic Scholar/arXiv) [2024-04-23]},
  file = {/home/philiposborne/Zotero/storage/ZFVAFCNE/Feng et al. - 2024 - Natural Language Reinforcement Learning.pdf}
}

@misc{luketinaSurveyReinforcementLearning2019,
  title = {A {{Survey}} of {{Reinforcement Learning Informed}} by {{Natural Language}}},
  author = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  year = {2019},
  month = jun,
  number = {arXiv:1906.03926},
  eprint = {1906.03926},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-26},
  abstract = {To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {248 citations (Semantic Scholar/arXiv) [2024-04-26]},
  note = {Comment: Published at IJCAI'19},
  file = {/home/philiposborne/Zotero/storage/HL2PXFE3/Luketina et al. - 2019 - A Survey of Reinforcement Learning Informed by Nat.pdf}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.5602},
  urldate = {2025-02-06},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: NIPS Deep Learning Workshop 2013},
  file = {/home/philiposborne/Zotero/storage/G3G9YTN3/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/philiposborne/Zotero/storage/AI58XTBY/1312.html}
}

@article{osborneSurveyTextGames2022,
  title = {A {{Survey}} of {{Text Games}} for {{Reinforcement Learning Informed}} by {{Natural Language}}},
  author = {Osborne, Philip and N{\~o}mm, Heido and Freitas, Andr{\'e}},
  editor = {Roark, Brian and Nenkova, Ani},
  year = {2022},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {873--887},
  doi = {10.1162/tacl_a_00495},
  urldate = {2023-11-02},
  abstract = {Reinforcement Learning has shown success in a number of complex virtual environments. However, many challenges still exist towards solving problems with natural language as a core component. Interactive Fiction Games (or Text Games) are one such problem type that offer a set of safe, partially observable environments where natural language is required as part of the Reinforcement Learning solution. Therefore, this survey's aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language. Specifically, this survey: 1) introduces the challenges in Text Game Reinforcement Learning problems, 2) outlines the generation tools for rendering Text Games and the subsequent environments generated, and 3) compares the agent architectures currently applied to provide a systematic review of benchmark methodologies and opportunities for future researchers.},
  keywords = {EvaluationProtocol,RL,Survey,TextGames},
  annotation = {11 citations (Semantic Scholar/DOI) [2024-04-23]},
  file = {/home/philiposborne/Zotero/storage/FJHK4J3F/Osborne et al. - 2022 - A Survey of Text Games for Reinforcement Learning .pdf}
}

@misc{ReinforcementLearning2018,
  title = {Reinforcement {{Learning}}},
  year = {2018},
  journal = {MIT Press},
  urldate = {2024-04-23},
  abstract = {Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from th...},
  howpublished = {https://mitpress.mit.edu/9780262193986/reinforcement-learning/},
  langid = {american},
  keywords = {Foundation,RL},
  file = {/home/philiposborne/Zotero/storage/3PFVZR7Q/reinforcement-learning.html}
}

@misc{shengCanLanguageAgents2023,
  title = {Can Language Agents Be Alternatives to {{PPO}}? {{A Preliminary Empirical Study On OpenAI Gym}}},
  shorttitle = {Can Language Agents Be Alternatives to {{PPO}}?},
  author = {Sheng, Junjie and Huang, Zixiao and Shen, Chuyun and Li, Wenhao and Hua, Yun and Jin, Bo and Zha, Hongyuan and Wang, Xiangfeng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03290},
  eprint = {2312.03290},
  publisher = {arXiv},
  urldate = {2024-04-24},
  abstract = {The formidable capacity for zero- or few-shot decision-making in language agents encourages us to pose a compelling question: Can language agents be alternatives to PPO agents in traditional sequential decision-making tasks? To investigate this, we first take environments collected in OpenAI Gym as our testbeds and ground them to textual environments that construct the TextGym simulator. This allows for straightforward and efficient comparisons between PPO agents and language agents, given the widespread adoption of OpenAI Gym. To ensure a fair and effective benchmarking, we introduce \$5\$ levels of scenario for accurate domain-knowledge controlling and a unified RL-inspired framework for language agents. Additionally, we propose an innovative explore-exploit-guided language (EXE) agent to solve tasks within TextGym. Through numerical experiments and ablation studies, we extract valuable insights into the decision-making capabilities of language agents and make a preliminary evaluation of their potential to be alternatives to PPO in classical sequential decision-making problems. This paper sheds light on the performance of language agents and paves the way for future research in this exciting domain. Our code is publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/mail-ecnu/Text-Gym-Agents\}.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Artificial Intelligence,Computation and Language,Language,RL},
  annotation = {1 citations (Semantic Scholar/arXiv) [2024-04-24]},
  file = {/home/philiposborne/Zotero/storage/XLWFVWZV/Sheng et al. - 2023 - Can language agents be alternatives to PPO A Prel.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement {{Learning An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = oct,
  publisher = {A Bradford Book},
  address = {Cambridge, MA, USA},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  isbn = {978-0-262-03924-6},
  keywords = {Foundation,RL},
  annotation = {2,142 citations (ACM Digital Library)[2024-04-23]}
}
