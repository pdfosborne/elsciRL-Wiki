\documentclass{article}
\input{header}
\addbibresource{bibliography.bib}
\title{}
\begin{document}
\maketitle
\section{Definition}
\label{loc:definition}
An agent must incorporate the long-term outcomes of actions into the calculations. A well known method for achieving this is Q-learning and defined by \hyperref[loc:references/books/suttonreinforcementlearningintroduction2018.statement]{suttonReinforcementLearningIntroduction2018} whereby calculations are updated based on the transitions from the current state-action pair to the next state.

Formally, the value of state-action pairs, $Q(s,a)$, are updated using the following calculation in equation \hyperref[loc:^156197]{(1)} after reaching every non-terminal state. If the next state is a terminal state, then this is fixed to $Q(s^\prime,a^\prime)=0$ and often a large reward is provided depending on the outcome. Over time, the numeric results of the long-term outcome propagates backwards to the earliest states in an episode such that the agent can make immediate actions that are not going to cause long-term issues. 
\subsubsection{Q-Learning Update Rule:}
\label{loc:definition.q:learning_update_rule:}
\begin{equation*}
\tag{1}
	Q^{new}(s,a)\leftarrow Q(s,a) + \alpha {\bigg (} r + \gamma \max_{a'}Q(s',a') - Q(s,a) {\bigg )}
\end{equation*}
^156197

where:
\begin{itemize}
\item $Q(s,a)$ is the value of state-action pair $s$,
\item $\alpha$ is the learning rate parameter,
\item $r$ is the immediate reward,
\item $\gamma$ is the discount factor parameter and,
\item $Q(s', a')$ is the value of action-pair in the next state taking the best known action.
\end{itemize}

The method is defined as \emph{off-policy} as the calculation is based on the maximum possible value that could be obtained in the next state rather than one based on an action selection policy. 
\printbibliography
\end{document}